View(result)
max(result$RHAT)
library(("blvmeval"))
library("blvmeval")
raneff_mu_list <- vector("list", Nnum)
raneff_cov_list <- vector("list", Nnum)
for (i in 1:Nnum) {
raneff_mu_list[[i]] <- c(
mean(samps[, paste0("mu[", i, "]")]),
mean(samps[, paste0("phi[", i, "]")])
)
raneff_cov_list[[i]] <- cov(samps[, c(paste0("mu[", i, "]"), paste0("phi[", i, "]"))])
}
View(raneff_mu_list)
View(raneff_cov_list)
# specify the function that computes the log joint probability for each unit
log_joint_i <- function(samples_s, data, i, Ngrid, nodes) {
# conditional likelihood
Nobs <- data$Nobs
Nnum <- data$N
Tnum <- Nobs/Nnum
y_i <- data$y[((i-1)*Tnum+1):(i*Tnum)]
x_i <- data$x[((i-1)*Tnum+1):(i*Tnum)]
predicted_y <- matrix(NA, nrow = Ngrid*Ngrid, ncol = Tnum)
log_con_t_i <- matrix(NA, nrow = Ngrid*Ngrid, ncol = Tnum)
log_con_i <- numeric(Ngrid*Ngrid) # log likelihood for all observations for person i, for each quadrature point
x_i_extended <- rep(x_i, times = Ngrid * Ngrid)
x_i_extended_mat <- matrix(x_i_extended, nrow = Ngrid * Ngrid, ncol = Tnum, byrow = TRUE)
y_i_extended <- rep(y_i, times = Ngrid * Ngrid)
y_i_extended_mat <- matrix(y_i_extended, nrow = Ngrid * Ngrid, ncol = Tnum, byrow = TRUE)
# Compute log conditional likelihood for each unit
predicted_y <- nodes[, 1] + nodes[, 2] * x_i_extended_mat
log_con_t_i <- dnorm(y_i_extended_mat, mean = predicted_y, sd = 1/sqrt(samples_s[["y_pre"]]), log = TRUE)
log_con_i <- rowSums(log_con_t_i)
# Compute log prior probability for latent variables (random effects)
raneff_i <- numeric(Ngrid*Ngrid)
sd_mu <- sqrt(1/samples_s[["tau_beta_0"]])
sd_phi <- sqrt(1/samples_s[["tau_beta_1"]])
mean <- c(samples_s[["beta_0"]], samples_s[["beta_1"]])
sigma <- matrix(c(sd_mu^2, sd_mu*samples_s[["rho"]]*sd_phi,
sd_mu*samples_s[["rho"]]*sd_phi, sd_phi^2), nrow = 2)
log_raneff_i <- mvtnorm::dmvnorm(nodes, mean, sigma, log = TRUE)
# Return the log joint probability for each unit
log_raneff_i + log_con_i
}
log_lik_result <- blvmeval::log_lik(samples = samps, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
blvmeval::calc_IC(log_lik_result, 1)
library(blvmeval)
library(bleval)
# unloadNamespace("bleval")
# remove.packages("bleval", lib = "D:/Software/R/R-4.2.2/library")
install.packages("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/blvmeval_0.0.0.9000.tar.gz",
repos = NULL, lib = "D:/Software/R/R-4.2.2/library")
# unloadNamespace("bleval")
# remove.packages("bleval", lib = "D:/Software/R/R-4.2.2/library")
install.packages("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/bleval_0.0.0.9000.tar.gz",
repos = NULL, lib = "D:/Software/R/R-4.2.2/library")
library(bleval)
log_lik_result <- blvmeval::log_lik(samples = samps, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
# specify the function that computes the log joint probability for each unit
log_joint_i <- function(samples_s, data, i, Ngrid, nodes) {
# conditional likelihood
Nobs <- data$Nobs
Nnum <- data$N
Tnum <- Nobs/Nnum
y_i <- data$y[((i-1)*Tnum+1):(i*Tnum)]
x_i <- data$x[((i-1)*Tnum+1):(i*Tnum)]
predicted_y <- matrix(NA, nrow = Ngrid*Ngrid, ncol = Tnum)
log_con_t_i <- matrix(NA, nrow = Ngrid*Ngrid, ncol = Tnum)
log_con_i <- numeric(Ngrid*Ngrid) # log likelihood for all observations for person i, for each quadrature point
x_i_extended <- rep(x_i, times = Ngrid * Ngrid)
x_i_extended_mat <- matrix(x_i_extended, nrow = Ngrid * Ngrid, ncol = Tnum, byrow = TRUE)
y_i_extended <- rep(y_i, times = Ngrid * Ngrid)
y_i_extended_mat <- matrix(y_i_extended, nrow = Ngrid * Ngrid, ncol = Tnum, byrow = TRUE)
# Compute log conditional likelihood for each unit
predicted_y <- nodes[, 1] + nodes[, 2] * x_i_extended_mat
log_con_t_i <- dnorm(y_i_extended_mat, mean = predicted_y, sd = 1/sqrt(samples_s[["y_pre"]]), log = TRUE)
log_con_i <- rowSums(log_con_t_i)
# Compute log prior probability for latent variables (random effects)
raneff_i <- numeric(Ngrid*Ngrid)
sd_mu <- sqrt(1/samples_s[["tau_beta_0"]])
sd_phi <- sqrt(1/samples_s[["tau_beta_1"]])
mean <- c(samples_s[["beta_0"]], samples_s[["beta_1"]])
sigma <- matrix(c(sd_mu^2, sd_mu*samples_s[["rho"]]*sd_phi,
sd_mu*samples_s[["rho"]]*sd_phi, sd_phi^2), nrow = 2)
log_raneff_i <- mvtnorm::dmvnorm(nodes, mean, sigma, log = TRUE)
# Return the log joint probability for each unit
log_raneff_i + log_con_i
}
log_lik_result <- bleval::log_lik(samples = samps, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
# unloadNamespace("bleval")
# remove.packages("bleval", lib = "D:/Software/R/R-4.2.2/library")
install.packages("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/bleval_0.0.0.9000.tar.gz",
repos = NULL, lib = "D:/Software/R/R-4.2.2/library")
library(bleval)
save.image("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/bleval/example/0314.RData")
unloadNamespace("bleval")
remove.packages("bleval", lib = "D:/Software/R/R-4.2.2/library")
remove.packages("blvmeval", lib = "D:/Software/R/R-4.2.2/library")
install.packages("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/bleval_0.0.0.9000.tar.gz",
repos = NULL, lib = "D:/Software/R/R-4.2.2/library")
log_lik_result <- bleval::log_lik(samples = samps, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
unloadNamespace("bleval")
remove.packages("bleval", lib = "D:/Software/R/R-4.2.2/library")
install.packages("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/bleval_0.0.0.9000.tar.gz",
repos = NULL, lib = "D:/Software/R/R-4.2.2/library")
log_lik_result <- bleval::log_lik(samples = samps, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
bleval::calc_IC(log_lik_result, 1)
summary(post)
View(result)
dim(samps)
remove.packages("blvmeval", lib = "D:/Software/R/R-4.2.2/library")
knitr::opts_chunk$set(echo = TRUE)
remove.packages("blvmeval", lib = "D:/Software/R/R-4.2.2/library")
remove.packages("bleval", lib = "D:/Software/R/R-4.2.2/library")
install.packages("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/bleval_0.0.0.9000.tar.gz",
repos = NULL, lib = "D:/Software/R/R-4.2.2/library")
bleval::log_lik
library(bleval, lib.loc = "D:/Software/R/R-4.2.2/library")
log_lik_result <- bleval::log_lik(samples = samps, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
load("0314rmd.RData")
log_lik_result <- bleval::log_lik(samples = samps, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
bleval::calc_IC(log_lik_result, 1)
log_prior <- function(samples_s) {
dgamma(samples_s[["y_pre"]], shape = 0.001, rate = 0.001, log = TRUE) +
dnorm(samples_s[["beta_0"]], mean = 0, sd = sqrt(1/0.01), log = TRUE) +
dnorm(samples_s[["beta_1"]], mean = 0, sd = sqrt(1/0.01), log = TRUE) +
dgamma(samples_s[["tau_beta_0"]], shape = 0.001, rate = 0.001, log = TRUE) +
dgamma(samples_s[["tau_beta_1"]], shape = 0.001, rate = 0.001, log = TRUE) +
dunif(samples_s[["rho"]], min = -1, max = 1, log = TRUE)
}
param_names <- c("beta_0", "beta_1", "y_pre", "tau_beta_0", "tau_beta_1", "rho")
lb <- c(rep(-Inf, 2), rep(0, 3), -1)
ub <- c(rep(Inf, 2), rep(Inf, 3), 1)
names(lb) <- param_names
names(ub) <- param_names
View(result)
View(samps)
log_lik_result
install.packages("htmltools")
install.packages("htmltools")
packageVersion("htmltools")
install.packages("htmltools", lib = "D:/Software/R/R-4.2.2/library")
install.packages("htmltools", lib = "D:/Software/R/R-4.2.2/library")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(MASS) # For multivariate normal distribution
library(dplyr)
set.seed(123)
Nnum <- 500  # Number of level-2 units
Tnum <- 50   # Number of level-1 units
beta <- c(0, 0.3) # Fixed components of random intercept and slope
rho <- 0.3   # Correlation between random intercept and slope
u_cov <- sqrt(1) * sqrt(0.09) * rho
u_sigma <- matrix(c(1, u_cov, u_cov, 0.09), nrow = 2)
bdata <- as.data.frame(mvrnorm(Nnum, mu = beta, Sigma = u_sigma))
names(bdata) <- c("mu", "phi")
bdata$ID <- 1:Nnum
mydata <- expand.grid(ID = 1:Nnum, t = 1:Tnum)
mydata$e_it <- rnorm(Nnum * Tnum, mean = 0, sd = 0.1)
mydata$x_it <- rnorm(Nnum * Tnum, mean = 0, sd = 1)
mydata <- left_join(mydata, bdata, by = "ID")
mydata$y_it <- mydata$mu + mydata$x_it * mydata$phi + mydata$e_it
summary(mydata)  # Summary of generated data
library(rjags)
model_string <- "
model {
# likelihood ------------------------------
for (j in 1:Nobs) {
y[j] ~ dnorm(y_mu[j], y_pre)
y_mu[j] <- mu[subject[j]] + phi[subject[j]] * x[j]
}
for (i in 1:N) {
mu[i]  <- raneff[i,1]
phi[i] <- raneff[i,2]
raneff[i,1:2] ~ dmnorm(beta[1:2], pre[1:2,1:2])
}
# priors ---------------------------------
y_pre ~ dgamma(0.001, 0.001)
log_pre <- log(y_pre)
beta[1] ~ dnorm(0, 0.01)
beta[2] ~ dnorm(0, 0.01)
beta_0 <- beta[1]
beta_1 <- beta[2]
tau_beta_0 ~ dgamma(0.001, 0.001)
sd_beta_0 <- sqrt(1/tau_beta_0)
tau_beta_1 ~ dgamma(0.001, 0.001)
sd_beta_1 <- sqrt(1/tau_beta_1)
rho ~ dunif(-1, 1)
sigma[1,1] <- sd_beta_0 * sd_beta_0
sigma[2,2] <- sd_beta_1 * sd_beta_1
sigma[1,2] <- sd_beta_0 * rho * sd_beta_1
sigma[2,1] <- sigma[1,2]
pre[1:2,1:2] <- inverse(sigma[1:2,1:2])
}
"
data_list <- list(
Nobs = nrow(mydata),
subject = mydata$ID,
N = length(unique(mydata$ID)),
y = mydata$y_it,
x = mydata$x_it
)
jags_model <- jags.model(textConnection(model_string), data = data_list, n.chains = 4)
update(jags_model, 5000)
variables <- c("mu", "phi",
"beta_0", "beta_1", "y_pre", "tau_beta_0", "tau_beta_1", "rho")
post <- coda.samples(jags_model, variable.names = variables, n.iter = 25000, thin = 10)
samps <- do.call(rbind, post)
dim(samps)
source("posterior_summary.R")
result <- summarize_posterior(post)
max(result$RHAT)
install.packages("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/bleval_0.0.0.9000.tar.gz",
repos = NULL, lib = "D:/Software/R/R-4.2.2/library")
# unloadNamespace("bleval")
# remove.packages("bleval", lib = "D:/Software/R/R-4.2.2/library")
library(bleval, lib.loc = "D:/Software/R/R-4.2.2/library")
log_joint_i <- function(samples_s, data, i, Ngrid, nodes) {
# Extract data for unit i
Nobs <- data$Nobs
Nnum <- data$N
Tnum <- Nobs / Nnum
y_i <- data$y[((i-1)*Tnum+1):(i*Tnum)]
x_i <- data$x[((i-1)*Tnum+1):(i*Tnum)]
# Expand x and y values for quadrature grid
x_i_extended_mat <- matrix(rep(x_i, times = Ngrid * Ngrid),
nrow = Ngrid * Ngrid, byrow = TRUE)
y_i_extended_mat <- matrix(rep(y_i, times = Ngrid * Ngrid),
nrow = Ngrid * Ngrid, byrow = TRUE)
# Compute log conditional likelihood for each unit
predicted_y <- nodes[, 1] + nodes[, 2] * x_i_extended_mat
log_con_t_i <- dnorm(y_i_extended_mat, mean = predicted_y,
sd = 1 / sqrt(samples_s[["y_pre"]]), log = TRUE)
log_con_i <- rowSums(log_con_t_i)
# Compute log prior probability for latent variables (random effects)
sd_mu <- sqrt(1 / samples_s[["tau_beta_0"]])
sd_phi <- sqrt(1 / samples_s[["tau_beta_1"]])
mean <- c(samples_s[["beta_0"]], samples_s[["beta_1"]])
sigma <- matrix(c(sd_mu^2, sd_mu * samples_s[["rho"]] * sd_phi,
sd_mu * samples_s[["rho"]] * sd_phi, sd_phi^2), nrow = 2)
log_raneff_i <- mvtnorm::dmvnorm(nodes, mean, sigma, log = TRUE)
# Return the log joint probability for each unit
log_raneff_i + log_con_i
}
# Create lists to store posterior means and covariance matrices of random effects
raneff_mu_list <- vector("list", Nnum)
raneff_cov_list <- vector("list", Nnum)
for (i in 1:Nnum) {
# Compute the posterior mean of the random intercept and slope for unit i
raneff_mu_list[[i]] <- c(
mean(samps[, paste0("mu[", i, "]")]),
mean(samps[, paste0("phi[", i, "]")])
)
# Compute the posterior covariance matrix of the random intercept and slope for unit i
raneff_cov_list[[i]] <- cov(samps[, c(paste0("mu[", i, "]"),
paste0("phi[", i, "]"))])
}
# Create lists to store posterior means and covariance matrices of random effects
raneff_mu_list <- vector("list", Nnum)
raneff_cov_list <- vector("list", Nnum)
for (i in 1:Nnum) {
# Compute the posterior mean of the random intercept and slope for unit i
raneff_mu_list[[i]] <- c(
mean(samps[, paste0("mu[", i, "]")]),
mean(samps[, paste0("phi[", i, "]")]))
# Compute the posterior covariance matrix of the random intercept and slope for unit i
raneff_cov_list[[i]] <- cov(samps[, c(paste0("mu[", i, "]"),
paste0("phi[", i, "]"))])
}
log_lik_result <- bleval::log_lik(samples = samps, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
bleval::calc_IC(log_lik_result, 1)
View(result)
log_joint_i <- function(samples_s, data, i, Ngrid, nodes) {
# Extract data for unit i
Nobs <- data$Nobs
Nnum <- data$N
Tnum <- Nobs / Nnum
y_i <- data$y[((i-1)*Tnum+1):(i*Tnum)]
x_i <- data$x[((i-1)*Tnum+1):(i*Tnum)]
# Expand x and y values for quadrature grid
x_i_extended_mat <- matrix(rep(x_i, times = Ngrid * Ngrid),
nrow = Ngrid * Ngrid, byrow = TRUE)
y_i_extended_mat <- matrix(rep(y_i, times = Ngrid * Ngrid),
nrow = Ngrid * Ngrid, byrow = TRUE)
# Compute log conditional likelihood for each unit
predicted_y <- nodes[, 1] + nodes[, 2] * x_i_extended_mat
log_con_t_i <- dnorm(y_i_extended_mat, mean = predicted_y,
sd = 1 / sqrt(samples_s[["y_pre"]]), log = TRUE)
log_con_i <- rowSums(log_con_t_i)
# Compute log prior probability for latent variables (random effects)
sd_mu <- sqrt(1 / samples_s[["tau_beta_0"]])
sd_phi <- sqrt(1 / samples_s[["tau_beta_1"]])
mean <- c(samples_s[["beta_0"]], samples_s[["beta_1"]])
sigma <- matrix(c(sd_mu^2, sd_mu * samples_s[["rho"]] * sd_phi,
sd_mu * samples_s[["rho"]] * sd_phi, sd_phi^2), nrow = 2)
log_raneff_i <- mvtnorm::dmvnorm(nodes, mean, sigma, log = TRUE)
# Return the log joint probability for each unit
log_raneff_i + log_con_i
}
# Create lists to store posterior means and covariance matrices of random effects
raneff_mu_list <- vector("list", Nnum)
raneff_cov_list <- vector("list", Nnum)
for (i in 1:Nnum) {
# Compute the posterior mean of the random intercept and slope for unit i
raneff_mu_list[[i]] <- c(
mean(samps[, paste0("mu[", i, "]")]),
mean(samps[, paste0("phi[", i, "]")]))
# Compute the posterior covariance matrix of the random intercept and slope for unit i
raneff_cov_list[[i]] <- cov(samps[, c(paste0("mu[", i, "]"),
paste0("phi[", i, "]"))])
}
pars_vector <- c("beta_0", "beta_1", "y_pre", "tau_beta_0", "tau_beta_1", "rho")
samps2 <- as.matrix(samps[ ,pars_vector])
dim(samps2)
log_lik_result <- bleval::log_lik(samples = samps2, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
bleval::calc_IC(log_lik_result, 1)
View(mydata)
knitr::opts_chunk$set(echo = TRUE)
library(MASS) # For multivariate normal distribution
library(dplyr)
set.seed(123)
Nnum <- 500  # Number of level-2 units
Tnum <- 50   # Number of level-1 units
beta <- c(0, 0.3) # Fixed components of random intercept and slope
rho <- 0.3   # Correlation between random intercept and slope
u_cov <- sqrt(1) * sqrt(0.09) * rho
u_sigma <- matrix(c(1, u_cov, u_cov, 0.09), nrow = 2)
bdata <- as.data.frame(mvrnorm(Nnum, mu = beta, Sigma = u_sigma))
names(bdata) <- c("mu", "phi")
bdata$ID <- 1:Nnum
mydata <- data.frame(matrix(NA, nrow = Nnum*Tnum, ncol = 2))
names(mydata) <- c("ID", "t")
mydata$ID <- rep(1:Nnum, each = Tnum)
mydata$t <- rep(1:Tnum, Nnum)
mydata$e_it <- rnorm(Nnum * Tnum, mean = 0, sd = 0.1)
mydata$x_it <- rnorm(Nnum * Tnum, mean = 0, sd = 1)
mydata <- left_join(mydata, bdata[, c("ID", "phi_i","mu_i")], by = "ID")
library(MASS) # For multivariate normal distribution
library(dplyr)
set.seed(123)
Nnum <- 500  # Number of level-2 units
Tnum <- 50   # Number of level-1 units
beta <- c(0, 0.3) # Fixed components of random intercept and slope
rho <- 0.3   # Correlation between random intercept and slope
u_cov <- sqrt(1) * sqrt(0.09) * rho
u_sigma <- matrix(c(1, u_cov, u_cov, 0.09), nrow = 2)
bdata <- as.data.frame(mvrnorm(Nnum, mu = beta, Sigma = u_sigma))
names(bdata) <- c("mu", "phi")
bdata$ID <- 1:Nnum
mydata <- data.frame(matrix(NA, nrow = Nnum*Tnum, ncol = 2))
names(mydata) <- c("ID", "t")
mydata$ID <- rep(1:Nnum, each = Tnum)
mydata$t <- rep(1:Tnum, Nnum)
mydata$e_it <- rnorm(Nnum * Tnum, mean = 0, sd = 0.1)
mydata$x_it <- rnorm(Nnum * Tnum, mean = 0, sd = 1)
mydata <- left_join(mydata, bdata[, c("ID", "phi","mu")], by = "ID")
mydata$y_it <- mydata$mu_i + mydata$x_it*mydata$phi_i + mydata$e_it
bdata <- as.data.frame(mvrnorm(Nnum, mu = beta, Sigma = u_sigma))
names(bdata) <- c("mu_i", "phi_i")
bdata$ID <- c(1:Nnum)
mydata <- data.frame(matrix(NA, nrow = Nnum*Tnum, ncol = 2))
names(mydata) <- c("ID", "t")
mydata$ID <- rep(1:Nnum, each = Tnum)
mydata$t <- rep(1:Tnum, Nnum)
mydata$e_it <- rnorm(n = Nnum*Tnum, mean = 0, sd = 0.1)
mydata$x_it <- rnorm(n = Nnum*Tnum, mean = 0, sd = 1)
mydata <- left_join(mydata, bdata[, c("ID", "phi_i","mu_i")], by = "ID")
mydata$y_it <- mydata$mu_i + mydata$x_it*mydata$phi_i + mydata$e_it
summary(mydata)  # Summary of generated data
library(rjags)
model_string <- "
model {
# likelihood ------------------------------
for (j in 1:Nobs) {
y[j] ~ dnorm(y_mu[j], y_pre)
y_mu[j] <- mu[subject[j]] + phi[subject[j]] * x[j]
}
for (i in 1:N) {
mu[i]  <- raneff[i,1]
phi[i] <- raneff[i,2]
raneff[i,1:2] ~ dmnorm(beta[1:2], pre[1:2,1:2])
}
# priors ---------------------------------
y_pre ~ dgamma(0.001, 0.001)
log_pre <- log(y_pre)
beta[1] ~ dnorm(0, 0.01)
beta[2] ~ dnorm(0, 0.01)
beta_0 <- beta[1]
beta_1 <- beta[2]
tau_beta_0 ~ dgamma(0.001, 0.001)
sd_beta_0 <- sqrt(1/tau_beta_0)
tau_beta_1 ~ dgamma(0.001, 0.001)
sd_beta_1 <- sqrt(1/tau_beta_1)
rho ~ dunif(-1, 1)
sigma[1,1] <- sd_beta_0 * sd_beta_0
sigma[2,2] <- sd_beta_1 * sd_beta_1
sigma[1,2] <- sd_beta_0 * rho * sd_beta_1
sigma[2,1] <- sigma[1,2]
pre[1:2,1:2] <- inverse(sigma[1:2,1:2])
}
"
data_list <- list(
Nobs = nrow(mydata),
subject = mydata$ID,
N = length(unique(mydata$ID)),
y = mydata$y_it,
x = mydata$x_it
)
# Initialize the JAGS model
jags_model <- jags.model(textConnection(model_string), data = data_list, n.chains = 4)
# Run the MCMC sampler with a burn-in period of 5000 iterations
update(jags_model, 5000)
# Draw posterior samples from the model
variables <- c("mu", "phi",
"beta_0", "beta_1", "y_pre", "tau_beta_0", "tau_beta_1", "rho")
post <- coda.samples(jags_model, variable.names = variables, n.iter = 25000, thin = 10)
# Combine posterior samples from all chains into a single matrix
samps <- do.call(rbind, post)
dim(samps)
# Extract posterior samples for model parameters only (excluding latent variables)
pars_vector <- c("beta_0", "beta_1", "y_pre", "tau_beta_0", "tau_beta_1", "rho")
samps2 <- as.matrix(samps[ ,pars_vector])
dim(samps2)
# Load the posterior_summary.R script to compute convergence diagnostics
source("posterior_summary.R")
# Summarize the posterior samples and compute the Gelman-Rubin diagnostic (R-hat)
result <- summarize_posterior(post)
# Compute the maximum R-hat
max(result$RHAT)
install.packages("D:/2_PhD/C_project/P15_marginal likelihood_tutorial/bleval/bleval_0.0.0.9000.tar.gz",
repos = NULL, lib = "D:/Software/R/R-4.2.2/library")
# unloadNamespace("bleval")
# remove.packages("bleval", lib = "D:/Software/R/R-4.2.2/library")
library(bleval, lib.loc = "D:/Software/R/R-4.2.2/library")
log_joint_i <- function(samples_s, data, i, Ngrid, nodes) {
# Extract data for unit i
Nobs <- data$Nobs
Nnum <- data$N
Tnum <- Nobs / Nnum
y_i <- data$y[((i-1)*Tnum+1):(i*Tnum)]
x_i <- data$x[((i-1)*Tnum+1):(i*Tnum)]
# Expand x and y values for quadrature grid
x_i_extended_mat <- matrix(rep(x_i, times = Ngrid * Ngrid),
nrow = Ngrid * Ngrid, byrow = TRUE)
y_i_extended_mat <- matrix(rep(y_i, times = Ngrid * Ngrid),
nrow = Ngrid * Ngrid, byrow = TRUE)
# Compute log conditional likelihood for each unit
predicted_y <- nodes[, 1] + nodes[, 2] * x_i_extended_mat
log_con_t_i <- dnorm(y_i_extended_mat, mean = predicted_y,
sd = 1 / sqrt(samples_s[["y_pre"]]), log = TRUE)
log_con_i <- rowSums(log_con_t_i)
# Compute log prior probability for latent variables (random effects)
sd_mu <- sqrt(1 / samples_s[["tau_beta_0"]])
sd_phi <- sqrt(1 / samples_s[["tau_beta_1"]])
mean <- c(samples_s[["beta_0"]], samples_s[["beta_1"]])
sigma <- matrix(c(sd_mu^2, sd_mu * samples_s[["rho"]] * sd_phi,
sd_mu * samples_s[["rho"]] * sd_phi, sd_phi^2), nrow = 2)
log_raneff_i <- mvtnorm::dmvnorm(nodes, mean, sigma, log = TRUE)
# Return the log joint probability for each unit
log_raneff_i + log_con_i
}
# Create lists to store posterior means and covariance matrices of random effects
raneff_mu_list <- vector("list", Nnum)
raneff_cov_list <- vector("list", Nnum)
for (i in 1:Nnum) {
# Compute the posterior mean of the random intercept and slope for unit i
raneff_mu_list[[i]] <- c(
mean(samps[, paste0("mu[", i, "]")]),
mean(samps[, paste0("phi[", i, "]")]))
# Compute the posterior covariance matrix of the random intercept and slope for unit i
raneff_cov_list[[i]] <- cov(samps[, c(paste0("mu[", i, "]"),
paste0("phi[", i, "]"))])
}
log_lik_result <- bleval::log_lik(samples = samps2, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i)
bleval::calc_IC(log_lik_result, 1)
log_prior <- function(samples_s) {
dgamma(samples_s[["y_pre"]], shape = 0.001, rate = 0.001, log = TRUE) +
dnorm(samples_s[["beta_0"]], mean = 0, sd = sqrt(1/0.01), log = TRUE) +
dnorm(samples_s[["beta_1"]], mean = 0, sd = sqrt(1/0.01), log = TRUE) +
dgamma(samples_s[["tau_beta_0"]], shape = 0.001, rate = 0.001, log = TRUE) +
dgamma(samples_s[["tau_beta_1"]], shape = 0.001, rate = 0.001, log = TRUE) +
dunif(samples_s[["rho"]], min = -1, max = 1, log = TRUE)
}
lb <- c(rep(-Inf, 2), rep(0, 3), -1)
ub <- c(rep(Inf, 2), rep(Inf, 3), 1)
names(lb) <- pars_vector
names(ub) <- pars_vector # c("beta_0", "beta_1", "y_pre", "tau_beta_0", "tau_beta_1", "rho")
bleval::log_marg_lik(samples = samps2, data = data_list, Ngrid = 9,
lv_mu = raneff_mu_list, lv_cov = raneff_cov_list,
log_joint_i = log_joint_i, log_prior = log_prior,
lb = lb, ub = ub)
